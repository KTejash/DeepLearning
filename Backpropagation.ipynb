{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbDS1ieq7fOabgWp4wr6rH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShubhamP1028/DeepLearningTute/blob/main/Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation\n",
        "Forward Pass :\n",
        "Input → Layer 1 → Activation → Layer 2 → Activation → Output → Loss\n",
        "\n",
        "<center><u>Backprop is exactly opposite to this\n",
        "</u>\n",
        "\n",
        "<b> weight (w) → raw neuron output (z) → final prediction (ŷ) → Loss (L) </b>\n",
        "\n",
        "</cemter>\n",
        "\n",
        "### Formula : ∂L/∂w = (∂L/∂ŷ) ⋅ (∂ŷ/∂z) ⋅ (∂z/∂w)\n",
        "\n",
        "Simply means that - The total impact of a weight on the final loss is the product of [how much the loss reacts to the prediction] times [how much the prediction reacts to the neuron's raw output] times [how much the neuron's raw output reacts to the weight]\n",
        "\n"
      ],
      "metadata": {
        "id": "0Df9U0cvI5hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "taking an example using 1 neuron\n",
        "\n",
        "1 Neuron:\n",
        "\n",
        "#### z=w⋅x+b\n",
        "#### ŷ =σ(z)\n",
        "##### L= 1/2 * ​(ŷ−y)2\n",
        "\n",
        "Now backprop: dw/dL = dŷ/dL ⋅ dz/dŷ ⋅ dw/dz\n",
        "​\n"
      ],
      "metadata": {
        "id": "oThBiHNTNGIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "\n",
        "### Building a mini neural net\n",
        "\n",
        "*  2 input neurons\n",
        "\n",
        "*  1 hidden layer (3 neurons, sigmoid)\n",
        "*  1 output neuron (sigmoid or softmax for multi-class)\n",
        "</center>"
      ],
      "metadata": {
        "id": "WoiV4OSbPOvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# without any deep learning library\n",
        "import numpy as np\n",
        "# Activation function\n",
        "def sigmoid(x): # bcs it return o or 1\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# This is the derivative of the sigmoid function.\n",
        "# We need this for backpropagation to calculate the gradient.\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# The Neural Network class that will encapsulate our model.\n",
        "class SimpleNeuralNetwork:\n",
        "    def __init__(self, input_nodes, hidden_nodes, output_nodes):\n",
        "        \"\"\"\n",
        "        Initializes the network's architecture and weights.\n",
        "        - input_nodes: 2\n",
        "        - hidden_nodes: 3\n",
        "        - output_nodes: 1\n",
        "        \"\"\"\n",
        "        # --- Initialize weights and biases with random values ---\n",
        "        # Weights connecting the input layer to the hidden layer (2x3 matrix)\n",
        "        self.weights_input_hidden = np.random.uniform(size=(input_nodes, hidden_nodes))\n",
        "        # Weights connecting the hidden layer to the output layer (3x1 matrix)\n",
        "        self.weights_hidden_output = np.random.uniform(size=(hidden_nodes, output_nodes))\n",
        "\n",
        "        # Biases for the hidden layer (3 neurons)\n",
        "        self.bias_hidden = np.random.uniform(size=(1, hidden_nodes))\n",
        "        # Bias for the output layer (1 neuron)\n",
        "        self.bias_output = np.random.uniform(size=(1, output_nodes))\n",
        "\n",
        "        print(\"Network initialized with random weights and biases.\")\n",
        "\n",
        "    def feedforward(self, inputs):\n",
        "        \"\"\"\n",
        "        Calculates the network's prediction for a given input.\n",
        "        This is the \"forward pass\".\n",
        "        \"\"\"\n",
        "        # 1. Calculate the signal into the hidden layer\n",
        "        hidden_layer_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden\n",
        "        # 2. Apply the activation function\n",
        "        hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "        # 3. Calculate the signal into the output layer\n",
        "        output_layer_input = np.dot(hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
        "        # 4. Apply the activation function to get the final prediction\n",
        "        predicted_output = sigmoid(output_layer_input)\n",
        "\n",
        "        return predicted_output, hidden_layer_output\n",
        "\n",
        "    def train(self, training_inputs, training_outputs, learning_rate, epochs):\n",
        "        \"\"\"\n",
        "        The main training loop. This is where the learning happens.\n",
        "        \"\"\"\n",
        "        print(f\"\\nStarting training for {epochs} epochs...\")\n",
        "        for epoch in range(epochs):\n",
        "            # --- Step 1: Forward Pass ---\n",
        "            predicted_output, hidden_layer_output = self.feedforward(training_inputs)\n",
        "\n",
        "            # --- Step 2: Calculate the Error ---\n",
        "            # This is (predicted - actual), our basic error signal\n",
        "            output_error = training_outputs - predicted_output\n",
        "\n",
        "            # --- Step 3: Backpropagation ---\n",
        "            # This is where we apply the chain rule to find the gradients.\n",
        "\n",
        "            # Gradient for the output layer\n",
        "            # This is (error * derivative_of_activation)\n",
        "            # This is our (ŷ - y) * ŷ(1 - ŷ) from the formula\n",
        "            output_delta = output_error * sigmoid_derivative(predicted_output)\n",
        "\n",
        "            # How much did the hidden layer contribute to the output error?\n",
        "            # Propagate the error backward to the hidden layer\n",
        "            hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
        "\n",
        "            # Gradient for the hidden layer\n",
        "            hidden_delta = hidden_error * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "            # --- Step 4: Update Weights and Biases (Gradient Descent) ---\n",
        "            # We move the weights in the opposite direction of their gradient.\n",
        "\n",
        "            # Update weights for hidden-to-output connections\n",
        "            self.weights_hidden_output += hidden_layer_output.T.dot(output_delta) * learning_rate\n",
        "            # Update weights for input-to-hidden connections\n",
        "            self.weights_input_hidden += training_inputs.T.dot(hidden_delta) * learning_rate\n",
        "\n",
        "            # Update biases\n",
        "            self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
        "            self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        \"\"\"\n",
        "        Make a prediction on new, unseen data.\n",
        "        \"\"\"\n",
        "        prediction, _ = self.feedforward(inputs)\n",
        "        return prediction\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# 1. Define the training data\n",
        "# We'll use the XOR problem, a classic non-linear problem.\n",
        "training_inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "training_outputs = np.array([[0], [1], [1], [0]]) # XOR outputs\n",
        "\n",
        "# 2. Create the neural network\n",
        "# 2 inputs, 3 hidden neurons, 1 output neuron\n",
        "nn = SimpleNeuralNetwork(input_nodes=2, hidden_nodes=3, output_nodes=1)\n",
        "\n",
        "# 3. Train the network\n",
        "# epochs: The number of times to loop through the entire training set.\n",
        "# learning_rate: How big of a step to take during gradient descent.\n",
        "nn.train(training_inputs, training_outputs, learning_rate=0.5, epochs=10000)\n",
        "\n",
        "# 4. Make predictions on the training data to see if it learned\n",
        "print(\"\\nPredictions after training:\")\n",
        "for i, input_data in enumerate(training_inputs):\n",
        "    print(f\"Input: {input_data},  Actual: {training_outputs[i][0]},  Predicted: {nn.predict(input_data)[0][0]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i6opaQgPCxr",
        "outputId": "79d8d16a-e634-4532-9cc6-ffd79300bb4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network initialized with random weights and biases.\n",
            "\n",
            "Starting training for 10000 epochs...\n",
            "Training complete!\n",
            "\n",
            "Predictions after training:\n",
            "Input: [0 0],  Actual: 0,  Predicted: 0.0148\n",
            "Input: [0 1],  Actual: 1,  Predicted: 0.9801\n",
            "Input: [1 0],  Actual: 1,  Predicted: 0.9899\n",
            "Input: [1 1],  Actual: 0,  Predicted: 0.0173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TLo0MIufIpcS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Tensorflow"
      ],
      "metadata": {
        "id": "KhtoW9Fxj0bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. The Model Architecture (Your code) ---\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=3, activation='sigmoid', input_shape=(2,)),\n",
        "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# --- 2. The Data ---\n",
        "# XOR problem: output is 1 if inputs are different, otherwise 0.\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n",
        "y = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "\n",
        "# --- 3. Compile the Model ---\n",
        "# This configures the model for training.\n",
        "# - optimizer='adam': Our efficient gradient descent algorithm.\n",
        "# - loss='binary_crossentropy': A loss function designed for binary (0 or 1)\n",
        "#   classification problems. It's the standard partner for a sigmoid output.\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled. Starting training...\\n\")\n",
        "\n",
        "# --- 4. Train the Model ---\n",
        "# - X, y: Our training data and labels.\n",
        "# - epochs=2000: The model will see the entire dataset 2000 times.\n",
        "# - verbose=0: Set to 0 to keep the output clean for this example.\n",
        "model.fit(X, y, epochs=2000, verbose=0)\n",
        "\n",
        "print(\"Training complete!\\n\")\n",
        "\n",
        "# --- 5. Make Predictions ---\n",
        "# Let's see how well it learned.\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions on training data:\")\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]}, Actual: {y[i][0]}, Predicted: {predictions[i][0]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jf6cyTOO2Ct",
        "outputId": "0db43b89-328d-4402-d646-a02aeddaa37f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled. Starting training...\n",
            "\n",
            "Training complete!\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "Predictions on training data:\n",
            "Input: [0. 0.], Actual: 0.0, Predicted: 0.2522\n",
            "Input: [0. 1.], Actual: 1.0, Predicted: 0.6163\n",
            "Input: [1. 0.], Actual: 1.0, Predicted: 0.8094\n",
            "Input: [1. 1.], Actual: 0.0, Predicted: 0.3499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Hidden Layer: 3 neurons\n",
        "    # Math for this layer (Forward Pass for one neuron):\n",
        "    # z1 = (input_1 * w1) + (input_2 * w2) + b\n",
        "    # h1 = sigmoid(z1)\n",
        "    # In matrix form for the whole layer: h = sigmoid(X · W1 + b1)\n",
        "\n",
        "# Output Layer: 1 neuron\n",
        "    # Math for this layer (Forward Pass):\n",
        "    # z2 = (h1 * w_h1) + (h2 * w_h2) + (h3 * w_h3) + b\n",
        "    # y_hat = sigmoid(z2)\n",
        "    # In matrix form: y_hat = sigmoid(h · W2 + b2)\n",
        "\n",
        "\n",
        "# 'adam': An efficient optimization algorithm (a form of Gradient Descent).\n",
        "# 'binary_crossentropy': The loss function.\n",
        "# Math for Loss (L): L = -[y*log(y_hat) + (1-y)*log(1-y_hat)]\n",
        "# This function heavily penalizes confident wrong predictions."
      ],
      "metadata": {
        "id": "mSrbp61RRYu-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The .fit() method encapsulates the training loop.\n",
        "# For each epoch, it performs the following for batches of data:\n",
        "#\n",
        "#   a) Forward Propagation:\n",
        "#      - The input data `X` is passed through the network to get a prediction `y_hat`.\n",
        "#      - This involves the matrix multiplications and sigmoid activations defined above.\n",
        "#\n",
        "#   b) Loss Calculation:\n",
        "#      - The `binary_crossentropy` loss `L` is calculated between the\n",
        "#        prediction `y_hat` and the true label `y`.\n",
        "#\n",
        "#   c) Backward Propagation (Backpropagation):\n",
        "#      - TensorFlow calculates the gradient of the Loss with respect to every\n",
        "#        weight and bias in the network. This is the chain rule in action.\n",
        "#      - e.g., For a weight `w` in the output layer:\n",
        "#        ∂L/∂w = (∂L/∂y_hat) * (∂y_hat/∂z2) * (∂z2/∂w)\n",
        "#\n",
        "#   d) Gradient Descent:\n",
        "#      - The 'adam' optimizer updates each weight and bias by moving it slightly\n",
        "#        in the opposite direction of its calculated gradient.\n",
        "#      - update = learning_rate * gradient\n",
        "#      - new_weight = old_weight - update"
      ],
      "metadata": {
        "id": "ETyKBdnhjmuN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "znCmQ55zjsYo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}