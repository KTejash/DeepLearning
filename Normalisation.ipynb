{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFRHA5/YoOe+MSaHkhXYcO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShubhamP1028/DeepLearning/blob/main/Normalisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalisation\n",
        "---\n",
        "We‚Äôll normalize the features so that each input variable is on a similar scale, which helps with convergence during training. I‚Äôll use MinMaxScaler for simplicity (you could also use StandardScaler).\n",
        "\n",
        "---\n",
        "<b>What is Normalisation? ‚û°</b>\n",
        "\n",
        "Imagine you‚Äôre training a group of athletes üèÉ‚Äç‚ôÄÔ∏èüèÉ‚Äç‚ôÇÔ∏è for a relay race.\n",
        "*  One is measured in meters per second,\n",
        "*  Another in minutes per kilometer,\n",
        "*  Another in miles per hour.\n",
        "\n",
        "If you don‚Äôt convert them into the same scale, comparing their performance is confusing.\n",
        "\n",
        "üëâ Normalization = putting all features on a common scale so the model can learn fairly.\n",
        "\n",
        "---\n",
        "<b>üõ† Why It Matters</b>\n",
        "*  Faster training ‚Äì Gradient descent works better when features have similar ranges.\n",
        "*  Avoid bias ‚Äì A large-scale feature (like salary in ‚Çπ100,000s) shouldn‚Äôt dominate a small-scale feature (like age in 20s).\n",
        "*  Better accuracy ‚Äì Helps the model find general patterns instead of latching onto magnitudes.\n",
        "\n",
        "Types of Normalization (most common)-\n",
        "\n",
        "<b>1. Min-Max Normalization (Scaling)</b>\n",
        "*  Rescales values into a fixed range, usually [0,1].\n",
        "$$x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$$\n",
        "\n",
        "*  Example:\n",
        "  *  Age 0‚Äì100 ‚Üí 0‚Äì1\n",
        "  *  Salary 20k‚Äì120k ‚Üí 0‚Äì1\n",
        "\n",
        "‚ö° Best when you know the min/max and data has no extreme outliers.\n",
        "\n",
        "---\n",
        "<b>2. Z-score Normalization (Standardization)</b>\n",
        "*  Centers data around 0 with standard deviation 1.\n",
        "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
        "*  Example: If height mean = 170cm, std = 10cm ‚Üí a person of 180cm =\n",
        "(180‚àí170)/10=1.0.\n",
        "\n",
        "Meaning: ‚Äú1 standard deviation above average.‚Äù\n",
        "\n",
        "‚ö° Best for ML/DL models (especially neural nets) since it keeps data balanced around zero.\n",
        "\n",
        "\n",
        "---\n",
        "3. Unit Vector Normalization\n",
        "*  Scale each row (sample) to length 1.\n",
        "$$x' = \\frac{x}{\\|x\\|}$$\n",
        "*  Useful in text embeddings, where direction matters more than magnitude.\n",
        "\n",
        "---\n",
        "## <u>Deep Learning Normalisation Techniques</u>\n",
        "Normalization happens before training (input features) and sometimes inside the network:\n",
        "*  Batch Normalization: Normalizes layer outputs during training to keep activations stable.\n",
        "*  Layer Normalization / Group Normalization: Variants for different architectures (like transformers).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HQRseB9TErIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <u>Batch Normalization (BN)</u>\n",
        "Intuition\n",
        "\n",
        "Normalizes activations across the batch dimension.\n",
        "\n",
        "Think: \"Look at all samples in the batch, compute mean & std, then normalize each feature channel.\"\n",
        "\n",
        "Very similar to Z-score normalization ‚Äî but applied per mini-batch during training.\n",
        "\n",
        "Formula-\n",
        "\n",
        "For activation\n",
        "ùë• in a batch:\n",
        "$$\\hat{x} = \\frac{x - \\mu_{\\text{batch}}}{\\sqrt{\\sigma^2_{\\text{batch}} + \\epsilon}}$$\n",
        "\n",
        "Then apply learnable scale and shift:\n",
        "$$y = \\gamma \\hat{x} + \\beta$$\n",
        "\n",
        "ùúá batch: mean of the batch\n",
        "\n",
        "ùúé batch2: variance of the batch\n",
        "\n",
        "ùúñ: small constant for stability\n",
        "\n",
        "ùõæ,ùõΩ: trainable parameters (so model can ‚Äúundo‚Äù normalization if useful)\n",
        "\n",
        "<u><b>Analogy to earlier normalizations</b></u>\n",
        "\n",
        "Like Z-score normalization, but mean/std are computed per batch, not globally.\n",
        "\n",
        "Keeps features stable across training steps."
      ],
      "metadata": {
        "id": "Zcn1zwGMIgZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhgNDJ3NEpVP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load dataset\n",
        "df = pd.read_csv(\"/mnt/data/goldstock.csv\")\n",
        "\n",
        "# 2. Feature engineering\n",
        "df['open-close'] = df['Open'] - df['Close']\n",
        "df['low-high'] = df['Low'] - df['High']\n",
        "df['is_quarter_end'] = np.where(df['Date'].str.endswith(('03-31','06-30','09-30','12-31')), 1, 0)\n",
        "\n",
        "# Target: Price goes up next day (binary classification)\n",
        "df['target'] = np.where(df['Close'].shift(-1) > df['Close'], 1, 0)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# 3. Features and target\n",
        "X = df[['open-close', 'low-high', 'is_quarter_end']].values\n",
        "y = df['target'].values\n",
        "\n",
        "# 4. Normalization\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 5. Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, shuffle=False\n",
        ")\n",
        "\n",
        "# 6. TensorFlow model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 7. Train the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=50,\n",
        "                    batch_size=16,\n",
        "                    verbose=1)\n",
        "\n",
        "# 8. Plot accuracy and loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 9. Evaluation\n",
        "loss, acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "print(f\"Validation Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "aHVoK1JXEql6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MIgqcAVNEqc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-tmqPFGwEqUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mjLRgwWfEqHL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}