{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLT08X7eFQ6rsmeeeCFa14",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShubhamP1028/DeepLearningTute/blob/main/Neuron_Layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZS9ByLnGXBlv"
      },
      "outputs": [],
      "source": [
        "# Loss Functions\n",
        "# Optimisers\n",
        "# back propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a Loss Function?\n",
        "Loss function or also called cost functions are basically model evaluating method or metrics. For example for a regression model Loss functions are MSE (Mean squard error ) and MAE (Mean absolute error), for classification model cross-entropy loss is loss function. And there values define how fit the predictions are.\n",
        "\n",
        "###  What are Optimisers?\n",
        "\n",
        "Optimisers are basically the methods or algorithms that are used to correct or redefine or make changes in parameters (weights and biases) during training to reduce loss function values and optimise th models performance.\n",
        "\n",
        "Types of Optimisers:\n",
        "*  **Gradient Descent** - A foundational optimization algorithm that updates parameters in the direction opposite to the gradient of the loss function with respect to the parameters.\n",
        "*  **Stochastic Gradient Descent (SGD)**: A variant of GD that updates parameters based on the gradient of the loss function computed for a single randomly selected training example at each step.\n",
        "\n",
        "# What is backpropagation?\n",
        "Backpropagation is an algorithm used to train artificial neural networks by adjusting the weights and biases of the network to minimize the difference between predicted and actual outputs. It works by calculating the gradient of the loss function (error) with respect to the network's weights and biases, then adjusting these parameters in the opposite direction of the gradient to reduce the error."
      ],
      "metadata": {
        "id": "Y8P_CWynduh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "svsQ91Pldt2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi neurons form a layer\n",
        "🧮 Input1, Input2 → Neuron 1 → Output 1\n",
        "\n",
        "          ↓→ Neuron 2 → Output 2\n",
        "\n",
        "          ↓→ Neuron 3 → Output 3"
      ],
      "metadata": {
        "id": "LIbwhFHfXPzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "activation functions"
      ],
      "metadata": {
        "id": "FeAWOq4hZiM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def leaky_relu(x):\n",
        "    return np.maximum(0.01*x, x)"
      ],
      "metadata": {
        "id": "CyYMpjQMXPEL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single data point with 2 features\n",
        "inputs = np.array([0.5, 0.8])"
      ],
      "metadata": {
        "id": "p6J3uuI5ZgX6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining th paramerters for 3 neurons"
      ],
      "metadata": {
        "id": "jB75CtNtZxts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 neurons, each with 2 weights (for 2 inputs)\n",
        "weights = np.array([\n",
        "    [0.2, 0.4],   # weights for neuron 1\n",
        "    [0.5, -0.3],  # weights for neuron 2\n",
        "    [-0.4, 0.9]   # weights for neuron 3\n",
        "])\n",
        "\n",
        "# Biases\n",
        "biases = np.array([0.1, -0.2, 0.05])\n"
      ],
      "metadata": {
        "id": "JNNVbw-AZrMT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "forward propagation"
      ],
      "metadata": {
        "id": "D2n42gl_aftz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dot product of inputs with weights\n",
        "weighted_sum = np.dot(weights, inputs) + biases"
      ],
      "metadata": {
        "id": "OzS7LhtFZ5kP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using activation functions to get output"
      ],
      "metadata": {
        "id": "n9F3McUHavSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid_output = sigmoid(weighted_sum)\n",
        "tanh_output = tanh(weighted_sum)\n",
        "relu_output = relu(weighted_sum)\n",
        "leaky_relu_output = leaky_relu(weighted_sum)"
      ],
      "metadata": {
        "id": "O7llCj95apMV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Sigmoid output: ', sigmoid_output)\n",
        "print('Tanh output: ', tanh_output)\n",
        "print('ReLU output: ', relu_output)\n",
        "print('Leaky ReLU output: ', leaky_relu_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ufabHUOa4Ip",
        "outputId": "490019db-b2ef-4172-d33e-780bf444092b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid output:  [0.62714777 0.45264238 0.63876318]\n",
            "Tanh output:  [ 0.47770001 -0.18774621  0.51535928]\n",
            "ReLU output:  [0.52 0.   0.57]\n",
            "Leaky ReLU output:  [ 0.52   -0.0019  0.57  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OTR4x_HvbJgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple class that mocks function of Keras. Layers. Dense"
      ],
      "metadata": {
        "id": "gQR-S6T7bmPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, n_inputs, n_neurons, activation):\n",
        "        self.weights = np.random.randn(n_neurons, n_inputs)\n",
        "        self.biases = np.random.randn(n_neurons)\n",
        "        self.activation = activation\n",
        "\n",
        "    def activate(self, x):\n",
        "        if self.activation == 'sigmoid':\n",
        "            return sigmoid(x)\n",
        "        elif self.activation == 'relu':\n",
        "            return relu(x)\n",
        "        else:\n",
        "            return x  # linear\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.z = np.dot(self.weights, inputs) + self.biases\n",
        "        return self.activate(self.z)\n",
        "\n",
        "# Example usage\n",
        "activation = input('Enter activation function to use: ')\n",
        "activation = activation.strip().lower()\n",
        "\n",
        "layer1 = DenseLayer(n_inputs=2, n_neurons=3, activation=activation)\n",
        "output = layer1.forward(np.array([0.5, 0.8]))\n",
        "print(\"Layer output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmRQPuKDbu6L",
        "outputId": "3f3948a8-43b4-4252-b2cf-b8fb5a1c44cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter activation function to use: sigmoid\n",
            "Layer output: [0.18996965 0.55096028 0.63626087]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JCXRqq8YcYYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi neuron layer using tensorflow.keras.Models. Squential"
      ],
      "metadata": {
        "id": "u_8-2w64csaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2JdHl6GdP0Z",
        "outputId": "4a0818d1-ef91-416a-ba34-c36648863686"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Dummy input data (2 features)\n",
        "X = np.array([[0.5, 1.0]])\n",
        "\n",
        "# Model with 1 layer of 3 neurons\n",
        "model = Sequential([\n",
        "    Dense(units=3, input_shape=(2,), activation='relu')\n",
        "])\n",
        "\n",
        "# Compile (won't train yet)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Predict (random weights for now)\n",
        "output = model.predict(X)\n",
        "print(\"Output from 3-neuron layer:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z-iKYP_dOIM",
        "outputId": "f24460e4-3906-411c-85da-cd6d93e5b9c2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\n",
            "Output from 3-neuron layer: [[1.359705   0.13681996 0.12005901]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bncCh9BHdTa9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}