{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKJr0Ja9PZSn+ZM4qosjhC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShubhamP1028/DeepLearning/blob/main/LearningRates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rates\n",
        "\n",
        "Learning rate scheduling helps the model converge faster and avoid bad minima. Instead of keeping LR constant, we reduce it gradually (or cyclically) during training.\n",
        "\n",
        "<b><u>what is Learning Rate?</u></b>\n",
        "*  The learning rate is like the ‚Äústep size‚Äù your model takes when learning.\n",
        "*  If it‚Äôs too big, the model jumps around and misses the best spot.\n",
        "*  If it‚Äôs too small, the model takes forever to reach the best spot.\n",
        "\n",
        "---\n",
        "<b><u>Why change the learning rate?</u></b>\n",
        "\n",
        "Think of climbing down a mountain:\n",
        "*  At the start, you want big steps to quickly cover distance.\n",
        "*  Near the bottom (the optimal point), you want small careful steps so you don‚Äôt overshoot.\n",
        "\n",
        "Learning rate schedules do exactly this:\n",
        "*  High LR at start ‚Üí fast learning.\n",
        "*  Low LR later ‚Üí fine-tuning.\n",
        "\n",
        "---\n",
        "### <u>Different Types of Learning Rate Schedules</u>\n",
        "<b>1. Step Decay</b>\n",
        "*  Learning rate drops suddenly after a fixed number of epochs.\n",
        "*  Example: Start at 0.1 ‚Üí after 10 epochs, reduce to 0.01 ‚Üí after 20 epochs, reduce to 0.001.\n",
        "*  üîé Simple but a bit ‚Äújumpy‚Äù.\n",
        "\n",
        "<b>2. Exponential Decay</b>\n",
        "*  Learning rate decreases smoothly over time, not suddenly.\n",
        "*  Formula:\n",
        "$$\n",
        "\\text{LR}(t) = LR_0 \\times e^{-\\text{decay_rate} \\times t}\n",
        "$$\n",
        "*  Example: If LR‚ÇÄ = 0.1 and decay_rate = 0.1 ‚Üí it shrinks gradually with each epoch.\n",
        "\n",
        "<b>3. Polynomial Decay</b>\n",
        "*  Decreases the LR following a polynomial curve (like 1/t¬≤).\n",
        "*  More flexible, can control how fast or slow LR reduces.\n",
        "\n",
        "<b>4. Cosine Annealing</b>\n",
        "*  Learning rate goes down like a wave (cosine curve).\n",
        "*  Starts high ‚Üí slowly drops ‚Üí almost zero.\n",
        "*  Looks smooth and natural.\n",
        "*  Sometimes resets (warm restarts) to encourage the model to explore new paths.\n",
        "\n",
        "<b>5. Cyclical Learning Rates (CLR)</b>\n",
        "*  Instead of only going down, LR goes up and down in cycles.\n",
        "*  Idea: small cycles help the model escape local traps and find better solutions.\n",
        "*  Example: oscillates between 0.001 and 0.01.\n",
        "\n",
        "<b>6. Warm-up</b>\n",
        "*  Start with a very small LR (so model doesn‚Äôt explode at the beginning).\n",
        "*  Gradually increase it to a bigger value.\n",
        "*  Often used with Transformers and very large networks.\n",
        "\n",
        "---\n",
        "<b>Summary in Simple Words</b>\n",
        "\n",
        "*  Learning rate schedule = changing the learning speed over time.\n",
        "*  High at start ‚Üí learn fast.\n",
        "*  Low at end ‚Üí learn carefully.\n",
        "\n",
        "Different strategies (step, exponential, cosine, cyclic, warm-up) are just different ways of controlling how the ‚Äúspeed‚Äù changes.\n",
        "\n",
        "---\n",
        "We‚Äôll implement common learning rate schedulers in PyTorch on your dataset:\n",
        "\n",
        "*  StepLR ‚Üí decreases LR every fixed number of epochs.\n",
        "*  ExponentialLR ‚Üí multiplies LR by a decay factor at each step.\n",
        "*  ReduceLROnPlateau ‚Üí reduces LR when validation loss stops improving.\n",
        "*  CosineAnnealingLR ‚Üí smooth cosine decay, popular in deep learning.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kYPtbtqCSbTg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOTT4D0uSCBb"
      },
      "outputs": [],
      "source": []
    }
  ]
}