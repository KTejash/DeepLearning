{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMICXgxThit4pQ1m8rQid4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Weight initialisation methods"
      ],
      "metadata": {
        "id": "U-7K0TbSlXlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training deep networks, how you initialize weights can make a huge difference. Why?\n",
        "*  If weights start too small, signals shrink as they move forward → vanishing gradients.\n",
        "*  If weights start too large, signals explode → exploding gradients.\n",
        "*  Both make training painfully slow or unstable.\n",
        "\n",
        "<b> {the term \"signal\" refers to the flow of information, During Forward Propagation: The \"signal\" is the output of a neuron, During Backpropagation:The \"signal\" is the gradient}</b>\n",
        "\n",
        "---\n",
        "\n",
        "<b> Vanishing Gradients</b>\n",
        "*  If weights are initialized too small, the signals (gradients) shrink exponentially as they propagate backward through the network.\n",
        "*  Result: Early layers receive tiny updates (their gradients approach zero), so they learn very slowly or stop learning entirely.\n",
        "  \n",
        "  Small weights → small gradients → multiplying many small numbers → near-zero gradients.\n",
        "---\n",
        "\n",
        "<b>Exploding Gradients</b>\n",
        "*  If weights are initialized too large, signals (gradients) grow exponentially during backpropagation.\n",
        "\n",
        "*  Result: Gradients become extremely large, causing:\n",
        "  *  Unstable training (loss fluctuates wildly).\n",
        "  *  Numerical overflow (e.g., NaN values).\n",
        "  *  Oversized weight updates that destabilize the model.\n"
      ],
      "metadata": {
        "id": "FUJzlxbxdwcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solutions\n",
        "<b>For Vanishing Gradients:</b>\n",
        "*  Use ReLU/Leaky ReLU activations (avoid sigmoid/tanh for deep networks).\n",
        "*  Batch Normalization stabilizes signal flow.\n",
        "*  Residual connections (e.g., in ResNet) allow gradients to bypass layers.\n",
        "---\n",
        "\n",
        "<b>For Exploding Gradients:</b>\n",
        "*  Gradient Clipping: Cap gradients during backpropagation.\n",
        "*  Weight Regularization (e.g., L2 regularization).\n",
        "*  Careful weight initialization (e.g., Xavier/Glorot for sigmoid, He initialization for ReLU).\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pePikNrRfayF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Initialization Methods\n",
        "<table>\n",
        "<tr><td>Method</td>\t<td>Formula</td><td>\tBest for</td><td>\tReason</td></tr>\n",
        "<tr><td>Xavier (Glorot)</td><td>\tVar(W) = 2 / (fan_in + fan_out) </td><td>\ttanh / sigmoid activations</td><td>\tBalances variance across layers</td></tr>\n",
        "<tr><td>He Initialization </td><td>\tVar(W) = 2 / fan_in\t</td><td>ReLU activations\t</td><td>Keeps variance high enough for ReLU’s sparse firing</td></tr>\n",
        "<tr><td>Uniform</td><td>\tSmall random values</td><td>\tOlder/simple models</td><td>\tRisk of vanishing/exploding</td></tr>\n",
        "<tr><td>Zeros</td><td>\tAll 0s</td><td>\t❌ Never use</td><td>\tNo symmetry breaking</td></tr>\n",
        "</table>\n",
        "\n",
        "*  fan_in = number of input neurons to the layer\n",
        "*  fan_out = number of output neurons from the layer\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nn9rVM-QcInq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')  # He init\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "model.apply(init_weights)\n"
      ],
      "metadata": {
        "id": "hKmmjIF2goX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kxnwpgluEXYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb1a547-bb0f-4a70-85c9-a17446af0030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9667\n"
          ]
        }
      ],
      "source": [
        "# PyTorch Full NN with He Initialization\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define model\n",
        "class FullNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.out = nn.Linear(32, 3)\n",
        "\n",
        "        # He Initialization\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
        "        nn.init.xavier_normal_(self.out.weight)  # output layer can use Xavier\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.out(x)\n",
        "\n",
        "model = FullNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Accuracy\n",
        "with torch.no_grad():\n",
        "    preds = torch.argmax(model(X_test), dim=1)\n",
        "    acc = (preds == y_test).float().mean()\n",
        "    print(f\"Test Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<b>PyTorch tensor ➡</b> is a multi-dimensional array (similar to NumPy's ndarray) that can store numerical data and perform fast computations on CPUs/GPUs. Tensors are the fundamental building blocks of PyTorch, used to encode inputs, outputs, and model parameters.\n",
        "\n",
        "Key Properties:\n",
        "*  Multi-dimensional: Scalars (0D), vectors (1D), matrices (2D), or higher-dimensional arrays.\n",
        "*  GPU Acceleration: Can be moved to CUDA-enabled GPUs for faster computation.\n",
        "*  Autograd Support: Track operations for automatic differentiation (critical for training neural networks).\n",
        "*  Optimized Operations: Backed by highly optimized C++/CUDA kernels.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jxgfMtIDiIHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorFlow Full NN with He Initialization\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess\n",
        "X, y = load_iris(return_X_y=True)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = tf.keras.utils.to_categorical(y, 3)  # one-hot encoding\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_normal', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax', kernel_initializer='glorot_normal')\n",
        "])\n",
        "\n",
        "# Compile & train\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, verbose=0)\n",
        "\n",
        "# Evaluate\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFOCPzQth1l8",
        "outputId": "9eae027d-1ca8-4641-e041-48567b7c063c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proper initialization = faster convergence + reduced vanishing/exploding."
      ],
      "metadata": {
        "id": "m3XmyuJojzLK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hxjbfXLFjjIb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}